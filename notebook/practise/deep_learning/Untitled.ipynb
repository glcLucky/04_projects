{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\06_software\\Anoconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义损失函数并求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.019347 ]\n",
      " [1.0428089]]\n"
     ]
    }
   ],
   "source": [
    "# 两个输入节点\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name='x-input')\n",
    "# 回归问题 一般只有一个输出节点\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name='y-input')\n",
    "\n",
    "# 定义一个单层的神经网络前向传播的过程\n",
    "w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "# 自定义损失函数\n",
    "loss_less = 10  # 预测少的的成本\n",
    "loss_more = 1  # 预测多的成本\n",
    "loss = tf.reduce_mean(tf.where(tf.greater(y, y_), (y - y_) * loss_more, (y_-y) * loss_less))\n",
    "\n",
    "# 用Adam求解损失函数最小值\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# 随机生成一个模拟数据集\n",
    "rdm = np.random.RandomState(1)\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size, 2)\n",
    "Y = [x1 + x2 + rdm.rand() / 10 - 0.05 for (x1, x2) in X]\n",
    "Y = np.array(Y).reshape(-1, 1)  # 转换成与y相同的形状\n",
    "\n",
    "# 训练神经网络\n",
    "batch_size = 8\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start + batch_size, dataset_size)\n",
    "        sess.run(train_step, feed_dict={x: X[start: end], y_: Y[start: end]})\n",
    "    print(sess.run(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.58651805]\n",
      " [ 0.98028255]\n",
      " [ 0.31031385]\n",
      " [ 0.48067787]\n",
      " [-1.30291   ]\n",
      " [ 0.32806247]\n",
      " [ 0.3879062 ]\n",
      " [ 0.0951909 ]\n",
      " [-0.06792744]\n",
      " [-0.5234168 ]]\n"
     ]
    }
   ],
   "source": [
    "def get_weight(shape, lamda):\n",
    "    \"\"\"\n",
    "    获取一层神经边上的权重 并将这个权重的L2正则化 损失加入名称为losses的集合中\n",
    "    \"\"\"\n",
    "    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(lamda)(var))\n",
    "    return var\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "batch_size = 8\n",
    "\n",
    "# 定义每一层网络中神经元个数\n",
    "layer_dimension = [2, 10, 10, 10, 1]\n",
    "# 神经网络的层数\n",
    "n_layers = len(layer_dimension)\n",
    "\n",
    "# 这个变量维护前向传播时最深层的节点 开始的时候就是输入层\n",
    "cur_layer = x\n",
    "# 当前层神经元个数\n",
    "in_dimension = layer_dimension[0]\n",
    "\n",
    "# 通过一个循环来生成5层全连接的神经网络结构\n",
    "for i in range(1, n_layers):\n",
    "    out_dimension = layer_dimension[i] # 下一层神经元个数\n",
    "    weight = get_weight([in_dimension, out_dimension], 0.001)\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[out_dimension]))\n",
    "    cur_layer = tf.nn.relu(tf.matmul(cur_layer, weight) + bias)\n",
    "    # 进入下一层之前将下一层神经元个数更新为当前层神经元个数\n",
    "    in_dimension = layer_dimension[i]\n",
    "\n",
    "# 在定义神经网络前向传播的同时已经将所有的L2正则化损失加入了图上的集合\n",
    "# 这里仅需计算刻画模型在训练集上表现的损失函数\n",
    "mse_loss = tf.reduce_mean(tf.square(y_ - cur_layer))\n",
    "\n",
    "# 将均方误差损失函数加入到损失集合\n",
    "tf.add_to_collection('losses', mse_loss)\n",
    "\n",
    "# get_collection返回一个列表 这个列表是所有这个集合中的元素\n",
    "loss = tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "# 用Adam求解损失函数最小值\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# 随机生成一个模拟数据集\n",
    "rdm = np.random.RandomState(1)\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size, 2)\n",
    "Y = [x1 + x2 + rdm.rand() / 10 - 0.05 for (x1, x2) in X]\n",
    "Y = np.array(Y).reshape(-1, 1)  # 转换成与y相同的形状\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# 训练神经网络\n",
    "batch_size = 8\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start + batch_size, dataset_size)\n",
    "        sess.run(train_step, feed_dict={x: X[start: end], y_: Y[start: end]})\n",
    "    print(sess.run(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-a227ec030b8c>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From E:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From E:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting C:\\Users\\youlin\\Desktop\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting C:\\Users\\youlin\\Desktop\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting C:\\Users\\youlin\\Desktop\\t10k-images-idx3-ubyte.gz\n",
      "Extracting C:\\Users\\youlin\\Desktop\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(r\"C:\\Users\\youlin\\Desktop\", one_hot=True)\n",
    "\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "LAYER1_NODE = 500\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DEACY = 0.99\n",
    "\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    \"\"\"\n",
    "    计算神经网络前向传播结果\n",
    "    \"\"\"\n",
    "    if avg_class == None:  # 没有提供滑动平均类时 直接使用参数当前的取值\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    else:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "\n",
    "# 训练模型的过程\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], 'y-input')\n",
    "    \n",
    "    # 生成隐层的参数\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "    \n",
    "    # 生成输出层的参数\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    # 计算在当前参数下神经网络前向传播的结果\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # 定义存储训练轮数的变量 这个变量不需要训练 所以指定为trainable=False\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 给定滑动平均衰减率和训练轮数变量 初始化滑动平均数\n",
    "    variables_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DEACY, global_step)\n",
    "    \n",
    "    # 在所有代表神经网络参数的变量上使用滑动平均\n",
    "    variables_averages_op = variables_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    # 计算使用了滑动平均之后的前向传播结果\n",
    "    average_y = inference(x, variables_averages, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.arg_max(y_, 1))\n",
    "    \n",
    "    # 计算在当前batch中所有样例的交叉熵平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 计算L2正则化损失函数\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    \n",
    "    # 计算总损失\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    # 设置指数衰减学习率\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, \n",
    "                                               global_step, \n",
    "                                               mnist.train.num_examples / BATCH_SIZE,\n",
    "                                              LEARNING_RATE_DECAY)\n",
    "    \n",
    "    # 选择优化方法\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # 在训练神经网络时 每过一遍数据既需要通过反向传播来更新神经网络的参数 又要更新每一个参数的滑动平均值\n",
    "    # 为了一次完成多个操作 可以使用tf.group\n",
    "    train_op = tf.group(train_step, variables_averages_op)\n",
    "    \n",
    "    # 计算正确率\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # 初始化会话并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        # 准备验证数据 一般在神经网络的训练过程中会通过验证数据来大致判断停止的条件和评判训练的效果\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "        \n",
    "        # 准备测试数据 在生产环境中 这部分数据在训练时是不可见的 这个数据只是作为模型优劣的最后评价标准\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "        \n",
    "        # 迭代地训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i % 1000 == 0:  # 每1000轮输出一次在验证集上的测试结果\n",
    "                # 由于验证集较小 没有分batch\n",
    "                validate_auc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After {} training step(s), validation accuracy using average model is {}\".format(i, validate_auc))\n",
    "            # 产生当前迭代的batch 并训练\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "        \n",
    "        # 在训练结束之后 在测试集上检测神经网络模型的最终正确率\n",
    "        test_auc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print(\"After {} training step(s), test accuracy using average model is {}\".format(TRAINING_STEPS, test_auc))\n",
    "    \n",
    "# 主程序入口\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(r\"C:\\Users\\youlin\\Desktop\", one_hot=True)\n",
    "    train(mnist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\youlin\\Desktop\\train-images-idx3-ubyte.gz\n",
      "Extracting C:\\Users\\youlin\\Desktop\\train-labels-idx1-ubyte.gz\n",
      "Extracting C:\\Users\\youlin\\Desktop\\t10k-images-idx3-ubyte.gz\n",
      "Extracting C:\\Users\\youlin\\Desktop\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-10-a227ec030b8c>:54: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "After 0 training step(s), validation accuracy using average model is 0.1509999930858612\n",
      "After 1000 training step(s), validation accuracy using average model is 0.9760000109672546\n",
      "After 2000 training step(s), validation accuracy using average model is 0.9825999736785889\n",
      "After 3000 training step(s), validation accuracy using average model is 0.9825999736785889\n",
      "After 4000 training step(s), validation accuracy using average model is 0.984000027179718\n",
      "After 5000 training step(s), validation accuracy using average model is 0.9846000075340271\n",
      "After 6000 training step(s), validation accuracy using average model is 0.9860000014305115\n",
      "After 7000 training step(s), validation accuracy using average model is 0.9842000007629395\n",
      "After 8000 training step(s), validation accuracy using average model is 0.984000027179718\n",
      "After 9000 training step(s), validation accuracy using average model is 0.9850000143051147\n",
      "After 10000 training step(s), validation accuracy using average model is 0.9847999811172485\n",
      "After 11000 training step(s), validation accuracy using average model is 0.9850000143051147\n",
      "After 12000 training step(s), validation accuracy using average model is 0.9847999811172485\n",
      "After 13000 training step(s), validation accuracy using average model is 0.9851999878883362\n",
      "After 14000 training step(s), validation accuracy using average model is 0.9846000075340271\n",
      "After 15000 training step(s), validation accuracy using average model is 0.9843999743461609\n",
      "After 16000 training step(s), validation accuracy using average model is 0.9850000143051147\n",
      "After 17000 training step(s), validation accuracy using average model is 0.9846000075340271\n",
      "After 18000 training step(s), validation accuracy using average model is 0.9851999878883362\n",
      "After 19000 training step(s), validation accuracy using average model is 0.9842000007629395\n",
      "After 20000 training step(s), validation accuracy using average model is 0.9851999878883362\n",
      "After 21000 training step(s), validation accuracy using average model is 0.9846000075340271\n",
      "After 22000 training step(s), validation accuracy using average model is 0.9855999946594238\n",
      "After 23000 training step(s), validation accuracy using average model is 0.9854000210762024\n",
      "After 24000 training step(s), validation accuracy using average model is 0.9854000210762024\n",
      "After 25000 training step(s), validation accuracy using average model is 0.9854000210762024\n",
      "After 26000 training step(s), validation accuracy using average model is 0.9850000143051147\n",
      "After 27000 training step(s), validation accuracy using average model is 0.9855999946594238\n",
      "After 28000 training step(s), validation accuracy using average model is 0.9854000210762024\n",
      "After 29000 training step(s), validation accuracy using average model is 0.9851999878883362\n",
      "After 30000 training step(s), test accuracy using average model is 0.9842000007629395\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1662: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.argmax(mnist.test.labels,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_tensor, avg_class, reuse=False):\n",
    "    \"\"\"\n",
    "    定义前向传播过程\n",
    "    \"\"\"\n",
    "    # 定义第一层神经网络的变量和前向传播过程\n",
    "    with tf.variable_scope('layer1', reuse=reuse):\n",
    "        weights = tf.get_variable(\"weights\", [INPUT_NODE, LAYER1_NODE], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        biases = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        if avg_class:\n",
    "            weights = avg_class.average(weights)\n",
    "            biases = avg_class.average(biases)\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "    \n",
    "    # 类似地定义第二层神经网络的变量和前向传播过程\n",
    "    with tf.variable_scope('layer2', reuse=reuse):\n",
    "        weights = tf.get_variable(\"weights\", [LAYER1_NODE, OUTPUT_NODE], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        biases = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        if avg_class:\n",
    "            weights = avg_class.average(weights)\n",
    "            biases = avg_class.average(biases)       \n",
    "        layer2 = tf.matmul(input_tensor, weights) + biases\n",
    "    # 返回前向传播结果\n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\youlin\\Desktop\\train-images-idx3-ubyte.gz\n",
      "Extracting C:\\Users\\youlin\\Desktop\\train-labels-idx1-ubyte.gz\n",
      "Extracting C:\\Users\\youlin\\Desktop\\t10k-images-idx3-ubyte.gz\n",
      "Extracting C:\\Users\\youlin\\Desktop\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(r\"C:\\Users\\youlin\\Desktop\", one_hot=True)\n",
    "\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "LAYER1_NODE = 500\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DEACY = 0.99\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def inference(input_tensor, avg_class, reuse=False):\n",
    "    \"\"\"\n",
    "    定义前向传播过程\n",
    "    \"\"\"\n",
    "    # 定义第一层神经网络的变量和前向传播过程\n",
    "    with tf.variable_scope('layer1', reuse=reuse):\n",
    "        weights = tf.get_variable(\"weights\", [INPUT_NODE, LAYER1_NODE], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        biases = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        if avg_class:\n",
    "            weights = avg_class.average(weights)\n",
    "            biases = avg_class.average(biases)\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "    \n",
    "    # 类似地定义第二层神经网络的变量和前向传播过程\n",
    "    with tf.variable_scope('layer2', reuse=reuse):\n",
    "        weights = tf.get_variable(\"weights\", [LAYER1_NODE, OUTPUT_NODE], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        biases = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        if avg_class:\n",
    "            weights = avg_class.average(weights)\n",
    "            biases = avg_class.average(biases)       \n",
    "        layer2 = tf.matmul(layer1, weights) + biases\n",
    "    # 返回前向传播结果\n",
    "    return layer2\n",
    "\n",
    "\n",
    "# 训练模型的过程\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], 'y-input')\n",
    "    \n",
    "    # 计算在当前参数下神经网络前向传播的结果\n",
    "    y = inference(x, None)\n",
    "    \n",
    "    # 定义存储训练轮数的变量 这个变量不需要训练 所以指定为trainable=False\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 给定滑动平均衰减率和训练轮数变量 初始化滑动平均数\n",
    "    variables_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DEACY, global_step)\n",
    "    \n",
    "    # 在所有代表神经网络参数的变量上使用滑动平均\n",
    "    variables_averages_op = variables_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    # 计算使用了滑动平均之后的前向传播结果\n",
    "    average_y = inference(x, variables_averages, True)\n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.arg_max(y_, 1))\n",
    "    \n",
    "    # 计算在当前batch中所有样例的交叉熵平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 计算L2正则化损失函数\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    with tf.variable_scope(\"\", reuse=True):\n",
    "        regularization = regularizer(tf.get_variable(\"layer1/weights\")) + regularizer(tf.get_variable(\"layer2/weights\"))\n",
    "    \n",
    "    # 计算总损失\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    # 设置指数衰减学习率\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, \n",
    "                                               global_step, \n",
    "                                               mnist.train.num_examples / BATCH_SIZE,\n",
    "                                              LEARNING_RATE_DECAY)\n",
    "    \n",
    "    # 选择优化方法\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # 在训练神经网络时 每过一遍数据既需要通过反向传播来更新神经网络的参数 又要更新每一个参数的滑动平均值\n",
    "    # 为了一次完成多个操作 可以使用tf.group\n",
    "    train_op = tf.group(train_step, variables_averages_op)\n",
    "    \n",
    "    # 计算正确率\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # 初始化会话并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        # 准备验证数据 一般在神经网络的训练过程中会通过验证数据来大致判断停止的条件和评判训练的效果\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "        \n",
    "        # 准备测试数据 在生产环境中 这部分数据在训练时是不可见的 这个数据只是作为模型优劣的最后评价标准\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "        \n",
    "        # 迭代地训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i % 1000 == 0:  # 每1000轮输出一次在验证集上的测试结果\n",
    "                # 由于验证集较小 没有分batch\n",
    "                validate_auc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After {} training step(s), validation accuracy using average model is {}\".format(i, validate_auc))\n",
    "            # 产生当前迭代的batch 并训练\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "        \n",
    "        # 在训练结束之后 在测试集上检测神经网络模型的最终正确率\n",
    "        test_auc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print(\"After {} training step(s), test accuracy using average model is {}\".format(TRAINING_STEPS, test_auc))\n",
    "    \n",
    "# 主程序入口\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(r\"C:\\Users\\youlin\\Desktop\", one_hot=True)\n",
    "    train(mnist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\youlin\\Desktop\\train-images-idx3-ubyte.gz\n",
      "Extracting C:\\Users\\youlin\\Desktop\\train-labels-idx1-ubyte.gz\n",
      "Extracting C:\\Users\\youlin\\Desktop\\t10k-images-idx3-ubyte.gz\n",
      "Extracting C:\\Users\\youlin\\Desktop\\t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step(s), validation accuracy using average model is 0.07280000299215317\n",
      "After 1000 training step(s), validation accuracy using average model is 0.9775999784469604\n",
      "After 2000 training step(s), validation accuracy using average model is 0.9811999797821045\n",
      "After 3000 training step(s), validation accuracy using average model is 0.9825999736785889\n",
      "After 4000 training step(s), validation accuracy using average model is 0.9847999811172485\n",
      "After 5000 training step(s), validation accuracy using average model is 0.9833999872207642\n",
      "After 6000 training step(s), validation accuracy using average model is 0.9842000007629395\n",
      "After 7000 training step(s), validation accuracy using average model is 0.9837999939918518\n",
      "After 8000 training step(s), validation accuracy using average model is 0.984000027179718\n",
      "After 9000 training step(s), validation accuracy using average model is 0.9842000007629395\n",
      "After 10000 training step(s), validation accuracy using average model is 0.9847999811172485\n",
      "After 11000 training step(s), validation accuracy using average model is 0.9847999811172485\n",
      "After 12000 training step(s), validation accuracy using average model is 0.9850000143051147\n",
      "After 13000 training step(s), validation accuracy using average model is 0.9847999811172485\n",
      "After 14000 training step(s), validation accuracy using average model is 0.9843999743461609\n",
      "After 15000 training step(s), validation accuracy using average model is 0.9850000143051147\n",
      "After 16000 training step(s), validation accuracy using average model is 0.9846000075340271\n",
      "After 17000 training step(s), validation accuracy using average model is 0.9847999811172485\n",
      "After 18000 training step(s), validation accuracy using average model is 0.9843999743461609\n",
      "After 19000 training step(s), validation accuracy using average model is 0.9846000075340271\n",
      "After 20000 training step(s), validation accuracy using average model is 0.9846000075340271\n",
      "After 21000 training step(s), validation accuracy using average model is 0.9847999811172485\n",
      "After 22000 training step(s), validation accuracy using average model is 0.9851999878883362\n",
      "After 23000 training step(s), validation accuracy using average model is 0.9854000210762024\n",
      "After 24000 training step(s), validation accuracy using average model is 0.9851999878883362\n",
      "After 25000 training step(s), validation accuracy using average model is 0.9851999878883362\n",
      "After 26000 training step(s), validation accuracy using average model is 0.9855999946594238\n",
      "After 27000 training step(s), validation accuracy using average model is 0.9855999946594238\n",
      "After 28000 training step(s), validation accuracy using average model is 0.9860000014305115\n",
      "After 29000 training step(s), validation accuracy using average model is 0.9854000210762024\n",
      "After 30000 training step(s), test accuracy using average model is 0.9832000136375427\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'temp1/v1:0' shape=(4,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"\", reuse=True):\n",
    "    print(tf.get_variable(\"temp1/v1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1662: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable temp1/v1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"E:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"E:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"E:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a1200ac2e5c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'temp1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"v1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"temp1/v1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1482\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1232\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mE:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    536\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 538\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mE:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    490\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    857\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 859\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    860\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable temp1/v1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"E:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"E:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"E:\\06_software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope('temp1'):\n",
    "    v1 = tf.get_variable(\"v1\", [4], initializer=tf.constant_initializer(3.0))\n",
    "print(tf.get_variable(\"temp1/v1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
