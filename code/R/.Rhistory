# 给定模型参数和ar的系数
b <- arima.sim(list(order = c(1,0,1), ar = 0.7 ,ma = 0.4), n = 100)
arma.acf = acf(b ,type = "correlation", plot = "T")
arma.acf
b <- arima.sim(list(order = c(1,0,1), ar = 0.7 ,ma = 0.4), n = 100)
acf(b ,type = "correlation", plot = "T")
b <- arima.sim(list(order = c(1,0,1), ar = 0.7 ,ma = 0.4), n = 100)
plot(b)
arma.acf = acf(b ,type = "correlation", plot = "T")
arma.acf
b <- arima.sim(list(order = c(1,0,1), ar = 0.7 ,ma = 0.4), n = 100)
plot(b)
arma.acf = acf(b ,type = "correlation", plot = "T")
arma.acf
b <- arima.sim(list(order = c(1,0,1), ar = 0.7 ,ma = 0.4), n = 100)
plot(b)
arma.acf = acf(b ,type = "correlation", plot = "T")
arma.acf
b <- arima.sim(list(order = c(1,0,1), ar = 0.7 ,ma = 0.4), n = 100)
plot(b)
arma.acf = acf(b ,type = "correlation", plot = "T")
arma.acf
arma.acf[1]
arma.acf[1:2]
b <- arima.sim(list(order = c(1,0,1), ar = 0.7 ,ma = 0.4), n = 100)
plot(b)
arma.acf = acf(b ,type = "correlation", plot = "T")
arma.acf[1:2]
help(arma)
arma.acf = acf(b ,type = "correlation", plot = "T" ,include.intercept = False)
arma.acf = acf(b ,type = "correlation", plot = "T" ,include.intercept = F)
arma.acf[1:2]
b
arma(b ,order = c(1,1),include.intercept = F, method = "ML")
arima(b ,order = c(1,0,1),include.intercept = F, method = "ML")
arima(b ,order = c(1,0,1), method = "ML")
arima(b ,order = c(1,0,1), include.mean = F, method = "ML")
arima(b ,order = c(1,0,1), include.mean = F, method = "CSS") # 条件最小二乘估计
help(arima.sim)
## 模拟数据并估计
#q4
ar.sim <- arima.sim(list(order = c(1,0,1), ar = 0.7 ,ma = 0.4 ), n = 100 )
## 模拟数据并估计
#q4
arma11.sim <- arima.sim(list(order = c(1,0,1), ar = 0.7 ,ma = 0.4 ), n = 100 )
help(ar2.sim)
help(arima.sim)
#q5
ar2.sim <- arima.sim(list(order = c(1,0,1), ar = c(1.5 , 0.75) ,ma = 0.4 ), n = 100 ,mean =100)
#q5
ar2.sim <- arima.sim(list(order = c(2,0,0), ar = c(1.5 , 0.75)), n = 100 ,mean =100)
help(ar.sim)
# 拟合ar模型
e = -rnorm(52)
ar.sim = filter(e ,filter = c(1.5 , 0.75) ,mean = 100,method = 'recursive')
help("filter")
ar.sim = filter(e ,filter = c(1.5 , 0.75) ,method = 'recursive')
ar.sim
mean(ar.sim)
mean(e)
ar.sim = filter(e ,filter = c(1.5 , 0.75) ,method = 'recursive')
mean(ar.sim)
ts.plot(ar.sim)
#q5
ar2.sim <- arima.sim(list(order = c(2,0,0), ar = c(1.5 , -0.75)), n = 100 ,mean =100)
ts.plot(ar2.sim)
mean(ar2.sim)
#q5
ar2.sim <- arima.sim(list(order = c(2,0,0), ar = c(1.5 , -0.75)), n = 100 )
mean(ar2.sim)
ts.plot(ar2.sim)
ar2.sim+100
mean(ar2.sim)
ar2.sim = ar2.sim+100
mean(ar2.sim)
ts.plot(ar2.sim)
arima(ar2.sim ,order = c(2,0,0),  method = "ML") # MLE
# 取出前40个值进行训练
ar2.sim.train = ar2.sim[:40]
# 取出前40个值进行训练
ar2.sim.train = ar2.sim[1:40]
ar2.sim.train
ar2.sim.test = ar2.sim[41:]
ar2.sim.test = ar2.sim[41:.]
ar2.sim.test = ar2.sim[41,]
ar2.sim.test = ar2.sim[41:52]
arima(ar2.sim.train ,order = c(2,0,0),  method = "ML") # MLE
fit = arima(ar2.sim.train ,order = c(2,0,0),  method = "ML") # MLE
#预测12期
predict=predict(fit,12)
predict
plot(predict$pred)
abline(0,mean(predict$pred))
mean(predict$pred)
abline(0,mean(predict$pred))
help(abline)
abline(h = mean(predict$pred))
abline(h = mean(predict$pred) ,cpl ='red')
abline(h = mean(predict$pred) ,c0l ='red')
abline(h = mean(predict$pred) ,col ='red')
abline(h = mean(predict$pred) ,col ='red')
abline(h = mean(predict$pred) ,c ='red')
plot(predict$pred)
abline(h = mean(predict$pred) ,c ='red')
abline(h = mean(predict$pred) ,c ='red')
abline(h = mean(predict$pred) ,col ='red')
plot(predict$pred)
abline(h = mean(predict$pred) ,col ='red')
plot(ar2.sim.train)
lines(predict$pred,col="red")
plot(ar2.sim.train ,type = "line")
plot(ar2.sim.train ,type = "l")
plot(ar2.sim.train ,type = "l")
lines(predict$pred,col="red")
#预测12期
predict=predict(fit,12)
lines(predict$pred,col="red")
predict$pred
# plot(ar2.sim.train ,type = "l")
plot(predict$pred)
#预测12期
forecast(fit ,h=12)
#预测12期
library(forecast)
library(forecast)
install.packages('forecast')
forecast(fit ,h=12)
#预测12期
library(forecast)
forecast(fit ,h=12)
forecast(fit ,h=12 ,plot = T)
pred = forecast(fit ,h=12)
plot(pred)
fit = arima(ar2.sim.train ,order = c(2,0,0),  method = "ML") # MLE
pred = forecast(fit ,h=12)
plot(pred)
ar2.sim.train
help(forecast)
fit = arima(ar2.sim.train ,order = c(2,0,0),  method = "ML") # MLE
pred = forecast(fit ,h=12)
pred
plot(pred)
plot(predict$pred)
plot(pred)
pred
ar2.sim.train
# 取出前40个值进行训练
ar2.sim.train <- ar2.sim[1:40]
ar2.sim.test <- ar2.sim[41:52]
fit = arima(ar2.sim.train ,order = c(2,0,0),  method = "ML") # MLE
pred = forecast(fit ,h=12)
plot(pred)
predict=predict(fit,12)
plot(pred$mean)
pred = forecast(fit ,h=12 ,level = c(0.025,0.975))
pred
lines(pred$lower ,col = 'red')
lines(pred$lower)
plot(pred$mean)
lines(pred$lower)
pred$lower
pred
pred$upper
pred$upper[0]
fit
pred$upper
plot(pred$upper)
lines(pred$mean)
lines(pred$mean)
predict=predict(fit,12)
predict
# plot(ar2.sim.train ,type = "l")
plot(predict$pred)
lines(predict$pred+predict$se,col="green")
lines(predict$pred-predict$se,col="green")
# plot(ar2.sim.train ,type = "l")
plot(predict$pred ,ylim =c(80,110) ,xlim = c(40,55))
lines(predict$pred+predict$se,col="green")
lines(predict$pred-predict$se,col="green")
lines(ar2.sim.test ,col='red')
fit.fore <- forecast(fit,h=12)
plot(fit.fore)
#q5
ar2.sim <- arima.sim(list(order = c(2,0,0), ar = c(1.5 , -0.75)), n = 100 ) # 模拟中心化ar2模型
ar2.sim = ar2.sim+100 # 加上均值
mean(ar2.sim)
ts.plot(ar2.sim)
# 取出前40个值进行训练
ar2.sim.train <- ar2.sim[1:40]
ar2.sim.test <- ar2.sim[41:52]
fit = arima(ar2.sim.train ,order = c(2,0,0),  method = "ML") # MLE
#预测12期
library(forecast)
pred = forecast(fit ,h=12 ,level = c(0.025,0.975))
plot(pred)
lines(pred$mean,col="red")
lines(pred$mean,col="red")
abline(h = mean(pred$mean) ,col ='red')
x <- c(41:52)
y <- ar2.sim[41:52]
points(x,y,col = "black")
#第一题
#车
car <- read.csv(file = "E:/04_project/R/car.csv",header = TRUE)
cormax <- cor(car[,-1])
cormax
carx <- car[,-1]
carx
cormax <- cor(car_x) # 生成自变量
car_x <- car[,-1]
cormax <- cor(car_x) # 生成自变量
pairs(~.,data = carx,main = "相关系数矩阵图")
#度量多重共线性的严重程度的一个重要指标是方阵的XTX的条件数，用kappa函数计算出条件数，大于1000时存在严重的多重共线性
kappa(cormax,exact = FALSE)
#λmax(XTX),λmin(XTX)表示的是XTX的最大,最小的特征值.
eigenvalue <- eigen(cormax)
eigenvalue
#特征值7、8、9、10、11均比较小，对应的7-11列特征向量可以找出存在共线性的变量
carfit <- lm(car$Y~.,data = car)
vif(carfit)
library("car")
vif(carfit)
#Ridge方法
plot(lm.ridge(car$Y~.,data = car,lambda = seq(0,1,0.0001)))
library('ridge')
#Ridge方法
plot(lm.ridge(car$Y~.,data = car,lambda = seq(0,1,0.0001)))
library(ridge)
library(car)
#Ridge方法
plot(lm.ridge(car$Y~.,data = car,lambda = seq(0,1,0.0001)))
install.packages('ridge')
install.packages("ridge")
library(ridge)
#Ridge方法
plot(lm.ridge(car$Y~.,data = car,lambda = seq(0,1,0.0001)))
library("ridge")
#Ridge方法
plot(lm.ridge(car$Y~.,data = car,lambda = seq(0,1,0.0001)))
cement <- data.frame(X1 = c(7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10),
X2 = c(26,29, 56, 31, 52, 55, 71, 31, 54, 47, 40, 66, 68),
X3 = c(6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8),
X4 = c(60, 52, 20, 47, 33, 22, 6, 44, 22, 26,34, 12, 12),
Y = c(78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7, 72.5, 93.1, 115.9, 83.8, 113.3, 109.4))
lm.sol <- lm(Y ~ ., data = cement)
summary(lm.sol)
ridge.sol <- lm.ridge(Y ~ ., lambda = seq(0, 1, length = 151), data = cement,
model = TRUE)
library(car)
#Ridge方法
plot(lm.ridge(car$Y~.,data = car,lambda = seq(0,1,0.0001)))
library(car)
library(ridge)
car <- read.csv(file = "E:/04_project/R/car.csv",header = TRUE)
car_x <- car[,-1]
cormax <- cor(car_x) # 生成自变量相关系数矩阵
#Ridge方法
plot(lm.ridge(car$Y~.,data = car,lambda = seq(0,1,0.0001)))
#特征值7、8、9、10、11均比较小，对应的7-11列特征向量可以找出存在共线性的变量
carfit <- lm(car$Y~.,data = car)
#Ridge方法
plot(lm.ridge(car$Y~.,data = car,lambda = seq(0,1,0.0001)))
library("ridge")
#Ridge方法
plot(lm.ridge(car$Y~.,data = car,lambda = seq(0,1,0.0001)))
install.packages('ridge')
install.packages("ridge")
library(ridge)
#车
car <- read.csv(file = "E:/04_project/R/car.csv",header = TRUE)
car_x <- car[,-1]
cormax <- cor(car_x) # 生成自变量相关系数矩阵
#Ridge方法
plot(lm.ridge(car$Y~.,data = car,lambda = seq(0,1,0.0001)))
library(MASS)
#Ridge方法
plot(lm.ridge(car$Y~.,data = car,lambda = seq(0,1,0.0001)))
#对应广义交叉验证对应的值最小GCV,手动拟合
select(lm.ridge(car$Y~.,data = car,lambda = seq(0,1,0.0001)))
modelcar<-lm.ridge(car$Y~.,data = car,lambda =1 )
summary(modelcar)
modelcar[1]
#对应广义交叉验证对应的值最小GCV,手动拟合
select(lm.ridge(car$Y~.,data = car,lambda = seq(0,1,0.0001)))
modelcar[1]
modelcar[0.01]
modelcar
cement <- data.frame(X1 = c(7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10),
X2 = c(26,29, 56, 31, 52, 55, 71, 31, 54, 47, 40, 66, 68),
X3 = c(6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8),
X4 = c(60, 52, 20, 47, 33, 22, 6, 44, 22, 26,34, 12, 12),
Y = c(78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7, 72.5, 93.1, 115.9, 83.8, 113.3, 109.4))
lm.sol <- lm(Y ~ ., data = cement)
summary(lm.sol)
# 从结果看，截距和自变量的相关系数均不显，但F检验却能通过 可能存在多重共线性问题
# 下面利用car包中的vif（）函数查看各自变量间的共线情况
library(car)
# 从结果看，各自变量的VIF值都超过10，存在多重共线性，其中，X2与X4的VIF值均超过200.
plot(X2 ~ X4, col = "red", data = cement) # 从散点图上可以发现x2和x4存在明显的负相关
#接下来，利用MASS包中的函数lm.ridge()来实现岭回归。下面的计算试了151个lambda值，最后选取了使得广义交叉验证GCV最小的那个
ridge.sol <- lm.ridge(Y ~ ., lambda = seq(0, 1, length = 151), data = cement,
model = TRUE)
library(car)
library(MASS)
df_reg_q9 <- read.csv("E:\\01_learn\\test data\\python\\reg_q9.csv",header=TRUE)
ln_df_reg_q9=log(df_reg_q9) #取对数
fit=lm(y~.,data = ln_df_reg_q9) # .表示对数据框内所有变量
summary(fit)
vif(fit) # 求方差膨胀因子判断多重共线性 vif<5 认为不存在共线性 5-10 中等程度 >10严重的多重共线性
View(ln_df_reg_q9)
View(ln_df_reg_q9)
ln_df_reg_q9_x = ln_df_reg_q9[,-1]
ln_df_reg_q9_x
# 画出自变量相关系数图
pairs(~.,data = ln_df_reg_q9_x,main = "相关系数矩阵图")
cormatrix <- cor(ln_df_reg_q9_x) # 生成自变量相关系数矩阵
#度量多重共线性的严重程度的一个重要指标是方阵的XTX的条件数，用kappa函数计算出条件数，大于1000时存在严重的多重共线性
kappa(cormatrix,exact = FALSE)
#第二题
#白血病
leuke <- read.csv(file = "E:/04_project/R/leukemia.csv",header = TRUE)
leuke
fit.leuke <- glm(leuke$Y~.,data = leuke,family = binomial())
summary(fit.leuke)
logitlink <- function(theta,x,y){
beta0 = theta[1]
beta1 = theta[2]
beta2 = theta[3]
beta3 = theta[4]
x1 = x[1]
x2 = x[2]
x3 = x[3]
p=1/(1+exp(-beta0-beta1*x1-beta2*x2-beta3*x3))
logL = sum(y*log(p)+(1-y)*log(1-p))
return(-logL)
leuke[,1:3]
optim(c(0,0,0,0), x = leuke[,1:3]
optim(c(0,0,0,0), x = leuke[,1:3]
leuke[,1:3]
#极大似然估计
logitlink <- function(theta,x,y){
beta0 = theta[1]
beta1 = theta[2]
beta2 = theta[3]
beta3 = theta[4]
x1 = x[1]
x2 = x[2]
x3 = x[3]
p=1/(1+exp(-beta0-beta1*x1-beta2*x2-beta3*x3))
logL = sum(y*log(p)+(1-y)*log(1-p))
return(-logL)
}
optim(c(0,0,0,0), x = leuke[,1:3],y = leuke[,4],logitlink)
#q5
ar2.sim <- arima.sim(list(order = c(2,0,0), ar = c(1.5 , -0.75)), n = 100 ) # 模拟中心化ar2模型
ar2.sim = ar2.sim+100 # 加上均值
mean(ar2.sim)
ts.plot(ar2.sim)
# 取出前40个值进行训练
ar2.sim.train <- ar2.sim[1:40]
ar2.sim.test <- ar2.sim[41:52]
#预测12期
library(forecast)
pred = forecast(fit ,h=12 ,level = c(0.025,0.975))
plot(pred)
pred = forecast(fit ,h=12 ,level = c(0.025,0.975))
fit = arima(ar2.sim.train ,order = c(2,0,0),  method = "ML") # MLE
pred = forecast(fit ,h=12 ,level = c(0.025,0.975))
plot(pred)
predict=predict(fit,12)
abline(h = mean(pred$mean) ,col ='red') # 在预测均值上画一条水平参考线
# 画出实际值的点图
x <- c(41:52)
y <- ar2.sim[41:52]
points(x,y,col = "black")
library(car)
library(MASS)
df_reg_q9 <- read.csv("E:\\01_learn\\test data\\python\\reg_q9.csv",header=TRUE)
ln_df_reg_q9=log(df_reg_q9) #取对数
ln_df_reg_q9_x = ln_df_reg_q9[,-1]
# 画出自变量相关系数图
pairs(~.,data = ln_df_reg_q9_x,main = "相关系数矩阵图")
cormatrix <- cor(ln_df_reg_q9_x) # 生成自变量相关系数矩阵
#度量多重共线性的严重程度的一个重要指标是方阵的XTX的条件数，用kappa函数计算出条件数，大于1000时存在严重的多重共线性
kappa(cormatrix,exact = FALSE)
fit=lm(y~.,data = ln_df_reg_q9) # .表示对数据框内所有变量
summary(fit)
vif(fit) # 求方差膨胀因子判断多重共线性 vif<5 认为不存在共线性 5-10 中等程度 >10严重的多重共线性
hatvalues(fit) #hii>(2p/n)被视为极端样本 p为显著的自变量个数(含constant)
#共线性解决方法之一 PCA
ln_df_reg_q9_std=scale(ln_df_reg_q9[,-1]) #将数据进行统计标准化 均值为0 方差为1
pca=eigen(cor(ln_df_reg_q9_std)) # 对中心化数据的相关阵进行特征值分解 或对非中心化数据的协方差阵进行特征值分解
pca$values #特征根 特征值之后即表示总方差 每个特征值表示该主成分所代表的变差部分
pca$vectors#特征向量 即主成分系数
cca=pca$values/sum(pca$values)#单个主成分的贡献率
cca
ca=cumsum(pca$values)/sum(pca$values)#累计贡献率
ca
#计算loading，即主成分与各原始变量之间的相关系数 2表示列方向的运算
loadings=sweep(pca$vectors,2,sqrt(pca$values),"*")
#计算每个样本的主成分得分 即用主成分系数乘以标准化后的样本值
p1=ln_df_reg_q9_std%*%pca$vectors[,1] #第一主成分
p2=ln_df_reg_q9_std%*%pca$vectors[,2] #第二主成分
y=ln_df_reg_q9_std[,1] #取出因变量y
new=data.frame(y,p1,p2) # 将因变量y与第一主成分放进同一个数据框
fit=lm(y~p1,data = new) # 将因变量y与第一主成分进行回归
summary(fit)
#new.res=resid(fit) # 获取普通残差
new.res_sd = stdres(fit) #获取标准化残差 等价于rstandard(fit)
plot(new$y, new.res_sd, ylab="Residuals", xlab="第一主成分",main="标准化残差图")
abline(0, 0)   #添加水平线
#估计置信区间
help(confint)
#估计置信区间
#help(confint)
confint(fit ,level = 0.99) #参数估计的置信区间 level位置信度 默认为0.95 即95%的置信区间
df_predict_data <- data.frame(p1=2) #prediction
predict(fit,df_predict_data,interval = 'confidence',alpha=0.05)
predict
cement <- data.frame(X1 = c(7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10),
X2 = c(26,29, 56, 31, 52, 55, 71, 31, 54, 47, 40, 66, 68),
X3 = c(6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8),
X4 = c(60, 52, 20, 47, 33, 22, 6, 44, 22, 26,34, 12, 12),
Y = c(78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7, 72.5, 93.1, 115.9, 83.8, 113.3, 109.4))
lm.sol <- lm(Y ~ ., data = cement)
summary(lm.sol)
# 从结果看，截距和自变量的相关系数均不显，但F检验却能通过 可能存在多重共线性问题
# 下面利用car包中的vif（）函数查看各自变量间的共线情况
library(car)
vif(lm.sol)
# 从结果看，各自变量的VIF值都超过10，存在多重共线性，其中，X2与X4的VIF值均超过200.
plot(X2 ~ X4, col = "red", data = cement) # 从散点图上可以发现x2和x4存在明显的负相关
#接下来，利用MASS包中的函数lm.ridge()来实现岭回归。下面的计算试了151个lambda值，最后选取了使得广义交叉验证GCV最小的那个
ridge.sol <- lm.ridge(Y ~ ., lambda = seq(0, 1, length = 151), data = cement,
model = TRUE)
names(ridge.sol)  # 变量名字
ridge.sol$lambda[which.min(ridge.sol$GCV)]  ##找到GCV最小时的lambdaGCV
## [1] 1
ridge.sol$coef[which.min(ridge.sol$GCV)]  ##找到GCV最小时对应的系数
par(mfrow = c(1, 2))
# 画出图形，并作出lambdaGCV取最小值时的那条竖直线
matplot(ridge.sol$lambda, t(ridge.sol$coef), xlab = expression(lamdba), ylab = "Cofficients",
type = "l", lty = 1:20)
abline(v = ridge.sol$lambda[which.min(ridge.sol$GCV)])
# 下面的语句绘出lambda同GCV之间关系的图形
plot(ridge.sol$lambda, ridge.sol$GCV, type = "l", xlab = expression(lambda),
ylab = expression(beta))
abline(v = ridge.sol$lambda[which.min(ridge.sol$GCV)])
ridge.sol$lambda[which.min(ridge.sol$GCV)] #最小GCV对应的lambda值是0.3267
# 下面利用ridge包中的linearRidge()函数岭回归参数 lambda=0.3267
library(ridge)
mod <- linearRidge(Y ~ ., data = cement ,lambda = 0.3267)
summary(mod)
# install.packages('lars')
library(lars)
x = as.matrix(cement[, 1:4])
y = as.matrix(cement[, 5])
(laa = lars(x, y, type = "lar"))  #lars函数值用于矩阵型数据
# 由此可见，LASSO的变量选择依次是X4，X1，X2，X3
plot(laa)  #绘出图
summary(laa)  #给出Cp值
## 逻辑回归
install.packages("AER")
data(Affairs, package="AER") # 导入数据集
head(Affairs)
summary(Affairs) # 查看数据集相关统计量
table(Affairs$affairs) # 统计affairs的取值个数
#将affair转换为二分类变量 命名为ynaffair
Affairs$ynaffair[Affairs$affairs > 0] <- 1 # 有婚外情
Affairs$ynaffair[Affairs$affairs == 0] <- 0 # 无婚外情
Affairs$ynaffair <- factor(Affairs$ynaffair,
levels=c(0,1),
labels=c("No","Yes"))
table(Affairs$ynaffair) # 没有婚外情的451例 有婚外情的 150例
# 逻辑回归建模
fit.full <- glm(ynaffair ~ gender + age + yearsmarried + children +
religiousness + education + occupation +rating,
data=Affairs, family=binomial(link = 'logit'))
summary(fit.full)
#剔除不显著的变量重新建立逻辑回归模型
fit.reduced <- glm(ynaffair ~ age + yearsmarried + religiousness +
rating, data=Affairs, family=binomial(link = 'logit'))
summary(fit.reduced)
# 利用anova比较fit.full和fit.reduced是否有显著差异
anova(fit.reduced, fit.full, test="Chisq")
#解释模型参数
coef(fit.reduced) #系数表示对数优势比 不好解释 因子进行指数化
exp(coef(fit.reduced)) # 得到优势比
## 时间序列 练习案例 尼罗河
plot(Nile,xlab = "年份" , ylab = "尼罗河流量")
acf(Nile,main = "自相关图")
tsdisplay(Nile) #也可以直接使用tsdisplay来观察，它包含了时序图，以及acf、pacf两个相关图
#adf检验
adf.test(Nile) #p-value=0.0642,故在95%的置信度下，无法拒绝不平稳的原假设，这与图示法结果一致
auto.arima(Nile, trace = T) #基于未差分的序列 最好的是arima(1,1,1)
# auto.arima(Nile_diff, trace = T) #基于未差分的序列 最好的是arima(1,0,1)
#最好基于未差分的  方便预测 不然预测的结果也是差分后的结果
fit = arima(Nile , order = c(1,1,1) , method = "ML") #ML 最大似然 CSS 条件最小二乘
#残差正态性检验
qqnorm(fit$residuals)
qqline(fit$residuals)
# 残差纯随机性检验
Box.test(fit$residuals , type = "Ljung-Box") # 接受纯随机的H0 即认为模型拟合的充分
#预测
pred = forecast(fit , h = 10)
plot(pred)
#预测
pred = forecast(fit , h = 10 ,level = c(0.025 , 0.075))
plot(pred)
#预测
pred = forecast(fit , h = 10 ,level = c(0.025 , 0.975))
plot(pred)
